<html><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> 
  <title>User-Centric Computing Group@UNNC</title> 
   
  <link href="./css/default.css" rel="stylesheet" type="text/css"> 
</head> 
 
<body> 
<div id="main-body"> 
 
<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
<!-- Header                                                            --> 
<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - --> 
 
<div id="header"> 
 
  <table> 
  <tbody><tr> 
    <td> 
      <h1>User-Centric Computing Group</h1>
      <p> 
        <b>1. Student-Centric</b> Undergraduate Research Society<br>
        <b>2. Student-First</b> Undergraduate Research Society<br>
        <b>3. Student-Independent</b> Undergraduate Research Society<br>
        The University of Nottingham Ningbo China<br> 
      </p>     
      <p> 
		<b>resources</b>  : <a href="./sites/faculty.html">Faculty Advisors</a> | <a href="./sites/members.html">Current Members</a> | <a href="./sites/pubs.html">Publications</a> | <a href="./sites/reflections.html">Reflections</a> | <a href="./sites/hiring.html">Join Us</a> | <a href="./sites/alumni.html">Alumni</a> | 
		</p> 
    </td> 
  </tr> 
  </tbody></table> 
 
</div> 
 
<div id="content"> 

<p>
<b>About Us: </b>  We are User-Centric Computing Group in The University of Nottingham Ningbo China, an undergraduate on-campus research society with a particular focus on digital computing technology. We have affiliated with multiple faculty advisors and their research groups, both internally and externally, to carry out interdisciplinary and (possibly) cutting-edge research outcome.
</p>
<p>
<b>Mission Statement: </b> The purpose of User-Centric Computing Group are three-folded: <br><b>1) Student-Centric. </b> All dedicated and sincere efforts from students shall be well-recognized and prompted. <br><b> 2) Student-First. </b> All innovative and interesting insights from students shall be properly guided and executed.<br><b> 3) Student-Independent. </b> All dilligent students with the right mindset shall be continually inspired and well-positioned.
</p>

<p>
<b>We Are Hiring! </b>  We are always looking for highly motivated, talented and dilligent students, who are with the right mindset, to join User-Centric Computing Group for semester/year-long projects. Please contact us if u are interested. Further information are <a href="./sites/hiring.html">here</a>. 
</p>


<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->
<!-- Research                                                          -->
<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->

<a name="research"></a>

<table class="two-col-h2"> <tbody><tr>
 <td class="left">
   <h2>Research Overview</h2>
 </tr> </tbody></table>

<p>
We are excited about <b>"User-Centric Revisits across the Whole Stack of Transformation Hierachy"</b>. On the one side, we attempt to have a broader view of computer system/architecture (cross-layer perspectives of Transformation Hierachy), with the end goal to improve and enhance user experience. On the other side, we aim to bridge user-centric concepts and metrics to generate, examine and evaluate the effects and influences of novel "black-box" arts and magics.
</p>

<p>
Our research has shifted from high-level applications to low-level micro-architecture, with the emphasis and focus on fast and space-efficient principles, which has been usually considered as two conflictable factors. The research approach, taken by us, is to incorporate theoratically efficient designs with practical prototypes, novel data supports and realistic studies/implications, which we aim to maximize its real-world impacts in potential and minimize the gaps for adaptions.
</p>

<div class="topics">

<!-- CHESTNUT - - - - - - - - - - - - - - - - - - - - - -->

<div class="topic"> <h3>CHESTNUT: Fast, Scalable and Extremely Serendipitous Recommender Systems</h3> <table> <tbody><tr>
  <td class="discussion">

The concept of serendipity has been understood narrowly and we have previously extended it from only "unexpectedness + usefulness" to "relevance + unexpectedness + usefulness". CHESTNUT, from the prototype we built during 2017 to 2019, has provided the opportunity to examine the effectivenss and practicality of such an approach. Compared with other development toolkits or external libraries, CHESTNUT is fast, scalable and extremely serendipitous among the largest data set publically. We enhance the performance of CHESTNUT via customized data model to handle more contexts, theoratically implicated significance weighting and Dynamic Programming. We announced CHESTNUT in <a href="./pdf/CHESTNUT_Invited-HCI-20.pdf">[HCI'20]</a> as an invited paper and more details are covered in our latest journal summary <a href="mailto:shiangjunpeng@gmail.com?subject=CHESTNUT Journal Preprint Request">[Preprint]</a>.

  </td> <td class="photo">
    <a href="http://github.com/unnc-ucc/CHESTNUT/">
      <br><img width="230" height="200" alt="CHESTNUT Overview Image" src="./img/CHESTNUT_Overview.jpg">
    </a><br> <a class="ref" href="http://github.com/unnc-ucc/CHESTNUT/">(CHESTNUT on Github)</a>
  </td>
</tr> </tbody></table> </div>

<!-- CFDCD - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->

<div class="topic"> <h3>Collaborative Filtering Directly on Compressed Documents: Presence and Future</h3>
<table> <tbody><tr>
  <td class="discussion">
	
	The key implication from CHESTNUT is that the Recommender Systems have started to utlize more context-based information, which was usually associated with the central building block of Collaborative Filtering - User-Item Matrix. Such demands have made us start to go beyond U-I Matrix and rethink the data infrastructure of Collaborative Filtering. As an alternative, we center Succinct Data Structures as the new data infrastructure, using Wavelet Tree as a case, to provide fast and space-efficient principles for future Recommender System Implementations. We examine the effectiveness through K-Nearest-Neighbor User-based Collaborative Filtering on 1,000 cores of Tianhe-II, and announced our case study in <a href="./pdf/SRCF-PDCAT-19.pdf">[PDCAT'19]</a>. We further wrote a summary article in terms of our relevant visions <a href="mailto:shiangjunpeng@gmail.com?subject=CFDCD Journal Preprint Request">[Preprint]</a>.

  

  </td> <td class="photo">
    <a href="mailto:shiangjunpeng@gmail.com?subject=CFDCD Journal Preprint Request">
      <br><img width="300" height="200" alt="CFDCD Overview" src="./img/CFDCD_Overview.png">
    </a><br> <a class="ref" href="mailto:shiangjunpeng@gmail.com?subject=CFDCD Journal Preprint Request">
      (Journal Summary Article)
    </a>
  </td>
</tr> </tbody></table> </div>


<!-- BROOK - - - - - - - - - - - - - - - - - - - - -->

<div class="topic"> <h3>BROOK: Multi-modal and Video Streams for Human-Vehicle Interaction Research</h3> <table> <tbody><tr>
  <td class="discussion">

	BROOK based on the key retrospection from the currently successful applications of Convolutional Neural Networks, and aims to address hidden challenges behind and make the first attempt for "domain-specific data support for Future Neural Networks". Unlike image classification databases, BROOK provides consistent data streams from as many angles (sensors, cameras, coordinates and etc.) as possible, which potentially helps researchers to build explainable designs, in the context of Human-Vehicle Interactions. <br> <br> We have applied OpenCV library and DenseNet models to examine the capability of speculative data estimators via facial videos, instead of equipping users with real sensors. We believe such a design choice would be capable to unleash the power of previous wisdoms on Human-Vehicle Interaction designs, by providing speculative and reliable data sources. In addition, BROOK is capable to allow interdisciplinary researchers to examine their proposals (e.g. Video Query and Analysis, Privacy Preserving and etc.). <br><br> We positioned BROOK in <a href="./pdf/BROOK-CHIW-20.pdf">[CHI'20]</a> workshop and announced our first prototype in <a href="mailto:shiangjunpeng@gmail.com?subject=Face2Multimodal Demo Preprint Request"> [Preprint]</a> in a later Interactivity. 

  </td> <td class="photo">
    <a href="./pdf/BROOK-CHIW-20.pdf">
      <br><img width="200" alt="BROOK Database Overview" src="./img/BROOK_Overview.jpg">
    </a><br> <a class="ref" href="https://unnc-ucc.github.io/BROOK/">
      (BROOK Project Website)
    </a>
  </td>
</tr> </tbody></table> </div>

<!-- Practical In-lab Simulations - - - - - - - - - - - - - - - - - - - - -->
<!--
<div class="topic"> <h3>O<sub>2</sub> for Driving Simulations: Toolkits for Agile Scenes + Scenarios Developments</h3> <table> <tbody><tr>
  <td class="discussion">

	We have learned the complexity and difficulty to design and implement the driving scenes and scenarios for simulations, through the hard way. The key issues has been folded by 1) implementation gap due to complicated interactions among different libraries; 2) unnecessary efforts due to relatively approximate design choices; and 3) workload unbalance between the programmers and other researchers. Denoted as project O<sub>2</sub>, both <a href="https://github.com/unnc-ucc/Onerios-OpenDS"> Onerios </a> and <a href="https://github.com/unnc-ucc/Omniverse-OpenDS"> Omniverse </a> aim to provide flexible, aigle and easy-to-use integration for scene and scenario generations. The key insight of O<sub>2</sub> is that we could offload code generations into automatic modes according to certain patterns. Both <a href="https://github.com/unnc-ucc/Onerios-OpenDS"> Onerios </a> and <a href="https://github.com/unnc-ucc/Omniverse-OpenDS"> Omniverse </a> would be announced in the near future, and we are also planning tutorials for others to learn.
  
  </td> <td class="photo">
    <a href="">
      <br><img width="200" alt="" src="">
    </a><br> <a class="ref" href="">
      ()
    </a>
  </td>
</tr> </tbody></table> </div>
-->
<!-- Serendipity Metrics - - - - - - - - - - - - - - - - - - - - -->

<div class="topic"> <h3>Examing Serendipitous Recommendations: The Missing Metrics for a Decade</h3> <table> <tbody><tr>
  <td class="discussion">

	The highly practical implementations of CHESTNUT have allowed us to carry out real-world characterizations. Hence, we have organized one-year user study, which consists of 104 participants on 11 campuses from 3 countries. Although the results have significantly supported the effectivenss of CHESTNUT, we do aware of the outstanding gap between off-line serendipity estimation and online serendipity feedbacks. Combining both real-world feedbacks and off-line results, we have addressed this challenge and further propose a statistical modelling approach for offline evaluations. We have announced our early findings from such a large-scale user study in <a href="./pdf/UserStudy-HCI-20.pdf">[HCI'20]</a> and summarize it with CHESTNUT in <a href="mailto:shiangjunpeng@gmail.com?subject=Serendipitous Algorithm Journal Preprint Request">[Preprint]</a>, and we would further introduce the latest breakthrough on off-line serendipity estimations.

  </td> <td class="photo">
    <a href="http://github.com/unnc-ucc/Early-Lessons-from-CHESTNUT">
      <br><img width="300" height="230" alt="Key Finding of CHESTNUT User Study" src="./img/CHESTNUT_UserStudy_Key.jpg">
    </a><br> <a class="ref" href="./pdf/UserStudy-HCI-20.pdf">
      (Early Findings of CHESTNUT User Study)
    </a>
  </td>
</tr> </tbody></table> </div>

<!-- Vision/Graphics - - - - - - - - - - - - - - - - - - - - -->

<div class="topic"> <h3>Cognitive Driving Simulators Enhancements: Retrospections Inside-out</h3> <table> <tbody><tr>
  <td class="discussion">

	We have been very interested in enhancing the performance of Cognitive Driving Simulators, with minimal modifications and optimizations. On the one hand, we have spent some time on examing the feasiblity of "restoring reality for in-lab simulations" through Video-to-video Synthesis tehcniques. We have positioned our vision on <a href="./pdf/Vid2vid-OpenDS-AutoUI-19.pdf">[AutomotiveUI'19]</a> Work-In-Progress. On the other hand, we have also encountered several internal issues while building BROOK database and we have been currently working on these issues. For example, the timing variation, caused by complex integration and complicated of different libraries, has been shown that it's a natural conflict with the key idea of BROOK, which would endanger its furtuer usages since time units are violated. 

  </td> <td class="photo">
    <a href="./pdf/Vid2vid-OpenDS-AutoUI-19.pdf">
      <br><img width="250" alt="Conceptual Vid2vid View from OpenDS" src="./img/vid2vid_Concept.jpg">
      <br><img width="250" alt="Tested Vid2vid View from OpenDS" src="./img/vid2vid_Test.jpg">
    </a><br> <a class="ref" href="./pdf/Vid2vid-OpenDS-AutoUI-19.pdf">
      (AutomotiveUI Position Paper) <br>
    </a>
	<a href="./pdf/BROOK-CHIW-20.pdf">
      <br><img width="250" alt="Conceptual Vid2vid View from OpenDS" src="./img/BROOK_Implication.jpg">
    </a><br> <a class="ref" href="./pdf/BROOK-CHIW-20.pdf">
	  (CHI Position Paper)
    </a>
  </td>
</tr> </tbody></table> </div>

<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->
<!-- Software                                                          -->
<!-- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -->

<h2>Open Source</h2>

<p>
We follow <a href="http://herpolhode.com/rob/utah2000.pdf"><b>the builder culture</b></a> and open source as much as we could. User-Centric Computing Group has implemented, collected and released several softwares and data sets on <a href="https://github.com/unnc-ucc"><b>[Github]</b></a>.
</p>

<div class="topics">
<ul class="tightlist">
  <li><a href="http://github.com/unnc-ucc/CHESTNUT/"><b>CHESTNUT</b></a>: CHESTNUT is the first practical user-based collaborative filtering system, with user-centr understanding of serendipity. CHESTNUT is fast, scalable and extremely serendipitous.
</li><br>
  <li><a href=""><b>BROOK v1.0</b></a>: BROOK is a series of databases for Human-Vehicle Interaction Research, with a special focus on emerging different kinds of data streams on the same timeline. The first version of BROOK is currently under review by the Ethics Committee.
</li><br>
  <li><a href="http://github.com/unnc-ucc/Early-Lessons-from-CHESTNUT/"><b>CHESTNUT User Study</b></a>: The large-scale user study has been carried out to evaluate CHESTNUT. The unified system has been studied with 104 real-world participants across 11 campuses from 3 countries, which substaintially supports its effectivenss and lead to several interesting implications for the future work.
</li><br>
  <li><a href="http://github.com/unnc-ucc/Document-OpenDS/"><b>OpenDS Documentation</b></a>: The original documentation of OpenDS leads to "cold-start" for inside-out modifications and re-implementations. Hence, we built a developer-centric documentation for future references, without requirements of programming expertises, to minimize the gaps of actual implementations.
</li><br>
  <li><a href="http://github.com/unnc-ucc/Scenes-OpenDS/"><b>OpenDS Scenes [2019 Version]</b></a>: We customized and implemented a series of driving scenes for in-lab simulations in 2019 summer. This has been considered as the key lesson to motivate our Onerios Project.
</li><br>
  <li><a href="http://github.com/unnc-ucc/Scenarios-OpenDS/"><b>OpenDS Scenarios [2019 Version]</b></a>: We customized and implemented a series of driving scenarios for in-lab simulations in 2019 summer. This has been considered as the key lesson to motivate our Omniverse Project.
</li><br>
  <li><a href="http://github.com/unnc-ucc/Attempt-Vid2Vid-OpenDS/"><b>Video-to-Video Synthesis in OpenDS</b></a>: We examined the feasiblity to restore reality for in-lab simulations through Video-to-Video Synethesis.
</li><br>
</ul>

</div> 

</div></div></body></html>